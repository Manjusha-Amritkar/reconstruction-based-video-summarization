# Reconstruction-Based Video Summarization

### BiLSTM Encoderâ€“Decoder with Selector Network for Unsupervised Video Summarization

---

## ðŸ“Œ Overview

This repository implements a **two-stage unsupervised video summarization framework** based on reconstruction-driven learning.

The method consists of:

1. A **BiLSTM encoderâ€“decoder (Reconstructor)** trained to reconstruct frame-level CNN features.
2. A **BiLSTM Selector network** trained using pseudo-labels derived from reconstruction error.
3. Threshold-based summary generation.
4. Evaluation on **SumMe** and **TVSum** benchmarks using Precision, Recall, and F1-score.

Unlike supervised approaches, this framework does **not require human-annotated labels during training**. Frame importance is inferred through reconstruction difficulty.

---

## ðŸ§  Methodology

### 1ï¸âƒ£ Feature Extraction

Video frames are processed using a pretrained **ResNet50** backbone:

* Input resolution: 224Ã—224
* Pooling: Global Average Pooling
* Output feature dimension: 2048

Frame features are stored as `.npy` files and used as input sequences.

---

### 2ï¸âƒ£ Stage I â€” Reconstruction Model

A 4-layer BiLSTM encoderâ€“decoder architecture is used:

**Encoder:**

* BiLSTM (hidden_dim)
* BiLSTM (hidden_dim / 2)

**Decoder:**

* BiLSTM (hidden_dim / 2)
* BiLSTM (hidden_dim)

The model is trained using Mean Squared Error (MSE):

```
L_recon = || X - X_hat ||^2
```

Where:

* `X` = original feature sequence
* `X_hat` = reconstructed features

Frames that are difficult to reconstruct tend to contain higher semantic variation.

---

### 3ï¸âƒ£ Pseudo Label Generation

Frame-level reconstruction error is computed:

```
e_i = mean((x_i - x_hat_i)^2)
```

The errors are:

* Normalized to [0,1]
* Slightly smoothed to avoid extreme targets
* Used as pseudo-importance labels

---

### 4ï¸âƒ£ Stage II â€” Selector Network

A BiLSTM-based selector predicts frame-level importance:

* BiLSTM (hidden_dim)
* Dense layer with sigmoid activation

Training loss:

```
L_selector = BinaryCrossEntropy(predicted_scores, pseudo_labels)
```

Where:

* Predicted scores are selector outputs
* Pseudo labels are normalized reconstruction errors

---

### 5ï¸âƒ£ Summary Generation

At inference time:

* Importance scores are computed
* Frames with score > threshold Ï„ are selected

Default threshold:

```
Ï„ = 0.6
```

This produces a binary summary without requiring knapsack optimization.

---

## ðŸ“Š Datasets

### ðŸ”¹ SumMe

* 25 consumer videos
* Multiple human-annotated summaries per video
* Evaluation via Precision, Recall, F1-score

### ðŸ”¹ TVSum

* 50 videos across 10 categories
* 20 user annotations per video
* Evaluation via average F1-score over users

âš  Datasets are **not included** due to licensing restrictions.

---

## ðŸ— Repository Structure

```
reconstruction-based-video-summarization/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ feature_extraction.py
â”‚   â”œâ”€â”€ reconstructor.py
â”‚   â”œâ”€â”€ selector.py
â”‚   â”œâ”€â”€ train_reconstructor.py
â”‚   â”œâ”€â”€ train_selector.py
â”‚   â”œâ”€â”€ summary_generator.py
â”‚   â”œâ”€â”€ evaluate_summe.py
â”‚   â”œâ”€â”€ evaluate_tvsum.py
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ data/
â”œâ”€â”€ models/
â”œâ”€â”€ results/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

```
pip install -r requirements.txt
```

---

## ðŸš€ How to Run

### Step 1 â€” Train Reconstructor

```
python src/train_reconstructor.py
```

---

### Step 2 â€” Train Selector

```
python src/train_selector.py
```

---

### Step 3 â€” Evaluate on SumMe

```
python src/evaluate_summe.py
```

---

### Step 4 â€” Evaluate on TVSum

```
python src/evaluate_tvsum.py
```

---

## ðŸ“ˆ Output

* Trained models saved in `models/`
* Evaluation results saved in `results/`
* Excel summaries generated for both datasets

---

## ðŸ§© Configuration

All parameters are centralized in:

```
src/config.py
```

Includes:

* Feature paths
* Annotation paths
* Model hyperparameters
* Training parameters
* Threshold
* Output paths

This design ensures portability and reproducibility.

---

## ðŸ”¬ Key Characteristics

âœ” Fully unsupervised
âœ” Two-stage training
âœ” Reconstruction-driven importance learning
âœ” No manual labeling required
âœ” Modular, reproducible architecture
âœ” Supports both SumMe and TVSum

---

## ðŸ“„ Citation

If you use this implementation, please cite:

```
Reconstruction-Based BiLSTM Selector for Unsupervised Video Summarization
```

(Replace with your formal citation once finalized.)

---

## ðŸ“œ License

This project is released under the MIT License.
>>>>>>> b82d44e (Add academic README with methodology, training, and evaluation details)
